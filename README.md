# ğŸ‡®ğŸ‡³ Maharashtra Agricultural Data ETL Pipeline

## ğŸ“Œ Project Goal
This project establishes a **fully automated, weekly ETL (Extract, Transform, Load) pipeline** for collecting and processing diverse agricultural data specific to the state of **Maharashtra, India**.

The ultimate goal is to produce a unified, **ML-ready dataset** in both **Parquet (for the data lake)** and **Excel (for analysis)** formats, featuring key derived features such as:

- **NPK Ratio**
- **Price-to-Cost Ratio**

---

## ğŸ—ï¸ Architecture Overview

The pipeline uses a modular approach, leveraging:

- **Python scripts** for orchestration and data ingestion
- **PySpark** for scalable data transformation and feature engineering

### ğŸ”„ Data Flow

#### **Extraction Layer (`etl_extractor.py`):**
- **AGMARKNET:** Simulated web scraping retrieves market arrivals, commodity, and price data.
- **Weather API:** Fetches real-time temperature, humidity, and rainfall data for key districts.
- **Deduplication:** Uses a SQLite database to track already extracted rows, ensuring idempotence.

#### **Transformation Layer (`etl_transformer.py`):**
- PySpark engine loads raw data and static lookups (Soil Health, Crop Metadata)
- Performs data cleansing, joins, standardization, and missing value imputation
- Executes feature engineering to prepare data for ML models

#### **Loading Layer**
- **Data Lake:** Saves processed Parquet files
- **Analytics:** Exports .xlsx format for human analysis

---

## ğŸš€ Local Setup and Execution

### **1. Prerequisites (Required for PySpark)**

| Software | Requirement |
|---------|-------------|
| Python | 3.9+ |
| Java | JDK/JRE (8 or 11) |
| Apache Spark | Spark 3.x installed |

---

### **2. Install Dependencies**

```sh
python -m venv AgriETLvenv
.\AgriETLvenv\Scripts\activate
pip install -r requirements.txt
```
### **3. Initialize Static Data**
```
python scripts/init_lookups.py
```

### **4. Execute Local Pipeline**
Extraction:
```
python scripts/etl_extractor.py
```
Transformation:
```
python scripts/etl_transformer.py
```

## â˜ï¸ Production Deployment (Docker & AWS Serverless)
### **1. Dockerize the Application**

Build Image:
```
docker build -t agri-etl-pipeline:latest .
```
### **2. Serverless Orchestration (Recommended)**

| Step        | Component       | Description |
|------------|-----------------|-------------|
| **Entrypoint** | `entrypoint.sh` | Executes extractor â†’ transformer |
| **Compute** | AWS Fargate | Runs containerized PySpark job |
| **Scheduler** | AWS EventBridge | Weekly cron trigger (`cron(0 0 ? * SUN *)`) |
| **Storage** | Amazon S3 | Stores processed Excel + Parquet outputs |


##ğŸ Final Output
âœ” Automated weekly ingestion
âœ” Unified cleaned ML dataset
âœ” Stored in both Parquet + Excel format
âœ” Ready for analysis and future ML model training

